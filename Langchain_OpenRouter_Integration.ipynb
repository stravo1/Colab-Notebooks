{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhOkMUKQWtvv4ud1RQ6Uig",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stravo1/Colab-Notebooks/blob/main/Langchain_OpenRouter_Integration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv openai langchain_core"
      ],
      "metadata": {
        "id": "YalNTiRMZuAw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97d7ffa8-3deb-4658-958e-185d47ec2690"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting openai\n",
            "  Downloading openai-1.14.3-py3-none-any.whl (262 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.9/262.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain_core\n",
            "  Downloading langchain_core-0.1.35-py3-none-any.whl (273 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.0/273.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (6.0.1)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain_core)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.0 (from langchain_core)\n",
            "  Downloading langsmith-0.1.34-py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging<24.0,>=23.2 (from langchain_core)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (8.2.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain_core)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain_core)\n",
            "  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_core) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_core) (2.0.7)\n",
            "Installing collected packages: python-dotenv, packaging, orjson, jsonpointer, h11, jsonpatch, httpcore, langsmith, httpx, openai, langchain_core\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonpatch-1.33 jsonpointer-2.4 langchain_core-0.1.35 langsmith-0.1.34 openai-1.14.3 orjson-3.9.15 packaging-23.2 python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "q96_dSyuZJxH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e26d0a02-4f08-4a2d-ba23-ca2e66f47fe1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()  # take environment variables from .env."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, List, Mapping, Optional, Union, Sequence, Tuple, Dict, Iterator\n",
        "\n",
        "from openai import OpenAI\n",
        "from os import getenv\n",
        "\n",
        "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
        "from langchain_core.language_models.llms import LLM\n",
        "from langchain_core.prompt_values import PromptValue\n",
        "from langchain_core.messages.base import BaseMessage\n",
        "from langchain_core.runnables.config import RunnableConfig"
      ],
      "metadata": {
        "id": "vMYGiOp2Zmp9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create an OpenAI API client\n",
        "client = OpenAI(\n",
        "  base_url=\"https://openrouter.ai/api/v1\",\n",
        "  api_key=getenv(\"API_KEY\"),\n",
        ")\n",
        "\n",
        "# more models at https://openrouter.ai/models\n",
        "model = \"google/gemma-7b-it:nitro\"\n",
        "\n",
        "system_prompt = {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"You are an AI assistant name Gemma.\",\n",
        "}\n",
        "\n",
        "# response is delivered at once instead of stremaing\n",
        "def get_completed_response(prompt):\n",
        "    completion = client.chat.completions.create(\n",
        "        model = model,\n",
        "        messages = [\n",
        "            system_prompt,\n",
        "            {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": prompt,\n",
        "            }\n",
        "          ],\n",
        "    )\n",
        "    return completion.choices[0].message.content\n",
        "\n",
        "# response is streamed\n",
        "def get_stream(prompt):\n",
        "    stream = client.chat.completions.create(\n",
        "        model = model,\n",
        "        stream = True,\n",
        "        temperature = 0.1,\n",
        "        messages = [\n",
        "            system_prompt,\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "    return stream;\n",
        "\n",
        "# inherit the LLM class from langchain\n",
        "class OpenRouterLLM(LLM):\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"open-router\"\n",
        "\n",
        "    # implementing the _call function is mandatory\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        return \"gg\"\n",
        "\n",
        "    def invoke(\n",
        "        self,\n",
        "        input: str,\n",
        "        config: Optional[RunnableConfig] = None, *,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        **kwargs: Any\n",
        "    ) -> str:\n",
        "        return get_completed_response(input)\n",
        "\n",
        "    def stream(\n",
        "        self,\n",
        "        input: str,\n",
        "        config: Optional[RunnableConfig] = None, *,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        **kwargs: Any\n",
        "        ) -> Iterator[str]:\n",
        "        return get_stream(input)"
      ],
      "metadata": {
        "id": "BRBtSOMTZqGv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenRouterLLM()\n",
        "\n",
        "print(llm.invoke(\"hi, what is your name?\"), \"\\n\")\n",
        "\n",
        "for chunk in llm.stream(\"Write me a song about sparkling water.\"):\n",
        "    print(chunk.choices[0].delta.content or \"\", end=\"\")"
      ],
      "metadata": {
        "id": "_2fJAokHZst8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f87f9da-5a8c-4856-aa85-2d969efe3546"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Gemma here, ready to assist you.**\n",
            "\n",
            "My name is Gemma. I am an AI assistant designed to provide you with information and help you with various tasks. Please let me know what I can assist you with today. \n",
            "\n",
            "(Verse 1)\n",
            "Bubbles dance, a bubbly spark,\n",
            "Sprightly fizz, a refreshing start.\n",
            "Water bubbles, clear as light,\n",
            "Sparkling joy, a pure delight.\n",
            "\n",
            "(Chorus)\n",
            "Oh, sparkling water, so refreshing,\n",
            "Your effervescence, a joy to hear.\n",
            "You quench my thirst, my soul's delight,\n",
            "And make my day oh so bright.\n",
            "\n",
            "(Verse 2)\n",
            "From lemon fizz to lime,\n",
            "You come in flavors, sweet and bright.\n",
            "With fruit, you dance and play,\n",
            "A bubbly treat, come what may.\n",
            "\n",
            "(Chorus)\n",
            "Oh, sparkling water, so refreshing,\n",
            "Your effervescence, a joy to hear.\n",
            "You quench my thirst, my soul's delight,\n",
            "And make my day oh so bright.\n",
            "\n",
            "(Bridge)\n",
            "Whether chilled or over ice,\n",
            "You bring a smile, a cooling breeze.\n",
            "In every sip, a bubbly dream,\n",
            "You make me feel, oh so keen.\n",
            "\n",
            "(Chorus)\n",
            "Oh, sparkling water, so refreshing,\n",
            "Your effervescence, a joy to hear.\n",
            "You quench my thirst, my soul's delight,\n",
            "And make my day oh so bright.\n",
            "\n",
            "(Outro)\n",
            "So raise a glass, let's toast to you,\n",
            "Sparkling water, forever true.\n",
            "May you sparkle, forever bright,\n",
            "And quench our thirst, day and night."
          ]
        }
      ]
    }
  ]
}