{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stravo1/Colab-Notebooks/blob/main/Ollama_Reddit_scrape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Amzljmr-U0Pp"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "subprocess.Popen([\"ollama\",\"serve\"])"
      ],
      "metadata": {
        "id": "LhjgoRV1WHQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subprocess.Popen([\"ollama\",\"run\", \"llama2\"])"
      ],
      "metadata": {
        "id": "97B-ru7gWRRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list"
      ],
      "metadata": {
        "id": "78e7mpPCbSVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "id": "4XRnoCfFcQW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import Ollama\n",
        "\n",
        "llm = Ollama(model=\"llama2\")\n",
        "\n",
        "query = '''\n",
        "hello world\n",
        "'''\n",
        "\n",
        "for chunks in llm.stream(query):\n",
        "    print(chunks, end=\"\")"
      ],
      "metadata": {
        "id": "x-WnXIzbcZMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_id(a):\n",
        "  for j in a.parents:\n",
        "    if j.name == \"div\" and (\"thing\" in j.get(\"class\")):\n",
        "      return j.get(\"data-fullname\")\n",
        "\n",
        "\n",
        "# Send a request to the Reddit page you want to scrape\n",
        "user_agent = \"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.1) Gecko/20071127 Firefox/2.0.1\"\n",
        "last_post_id = \"\"\n",
        "count = 0\n",
        "list_of_all_posts = []\n",
        "list_of_all_posts_text = \"\"\n",
        "\n",
        "for i in range(5):\n",
        "  url = \"https://old.reddit.com/r/india/\" + (\"?count=\" + count if count != 0 else \"\") + (\"&after=\" + last_post_id if count != 0 else \"\")\n",
        "  response = requests.get(url, headers={\"User-Agent\": user_agent})\n",
        "\n",
        "  # Parse the HTML content using Beautiful Soup\n",
        "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "  for i in soup.find_all(\"a\", \"title\"):\n",
        "    last_post_id = get_id(i)\n",
        "    list_of_all_posts.append(i.get_text())\n",
        "    list_of_all_posts_text += \"\\n\" + i.get_text()\n",
        "\n",
        "print(len(list_of_all_posts))"
      ],
      "metadata": {
        "id": "kfI5uKaDqaLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def scrape_wiki(url):\n",
        "    \"\"\"\n",
        "    Send a GET request to the Wikipedia page and parse the HTML content using Beautiful Soup.\n",
        "    Find the specific section of the page you want to scrape (e.g. the \"Features\" section) and extract the text content.\n",
        "    :param url: The URL of the Wikipedia page to scrape\n",
        "    :return: The text content of the specified section of the page\n",
        "    \"\"\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find the specific section of the page you want to scrape (e.g. the \"Features\" section)\n",
        "    content_list = soup.find('div', {'class': 'mw-parser-output'}).find_all_next(\"p\")\n",
        "\n",
        "    # Extract the text content of each item in the list\n",
        "    content = \"\"\n",
        "    for info in content_list:\n",
        "        content += info.get_text()\n",
        "\n",
        "    return content"
      ],
      "metadata": {
        "id": "Fvd-bhmyvuAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import Ollama\n",
        "import time\n",
        "from builtins import input\n",
        "\n",
        "llm = Ollama(model=\"llama2\", temperature=\"0.2\")\n",
        "\n",
        "query = \"following is a list of posts from india subreddits page \\n\" + list_of_all_posts_text + \"\\n summarize \"\n",
        "\n",
        "start_time = time.time()\n",
        "no_of_tokens = 0\n",
        "\n",
        "for chunks in llm.stream(query):\n",
        "    print(chunks, end=\"\")\n",
        "    no_of_tokens+=1\n",
        "\n",
        "end_time = time.time()\n",
        "print(\"\\n\\n----------\\n\",no_of_tokens / (end_time - start_time), \" tokens/s\")\n",
        "print(\"Total number of tokens: \", no_of_tokens)\n",
        "print(\"Total time taken: \", (end_time - start_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "810e9da9-0a38-4211-e426-8b959ea81267",
        "id": "SmZRSJ7vd9zI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is a summary of the latest updates from India:\n",
            "\n",
            "1. Varanasi: Bhujialalji-bikaji Bhujia contaminated with Machine grease or oil.\n",
            "2. Delhi: Arvind Kejriwal claims he should get a Nobel prize for running the government in Delhi.\n",
            "3. Gangtok, Sikkim: A team of fact-finders was briefly arrested en route to meet victims of alleged human trafficking.\n",
            "4. Assam: A bill that invited the ire of Christian groups has been tabled in the Assam Assembly.\n",
            "5. Kota: Teachers have been suspended for conducting religious conversions.\n",
            "6. Chandigarh: The BJP is fretting as repolls loom in the city and has shifted its councillors to a resort where even mobile signals are weak.\n",
            "7. Mumbai, 2024: A beautiful sunrise at Mahabalipuram.\n",
            "8. Haryana: The INLD unit chief Nafe Singh Rathee was shot dead in Jhajjar.\n",
            "9. Personal: A 22-year-old shares their struggles with crying over small issues and feeling like a complete loser.\n",
            "10. Education: The government has clipped the powers of the IIM Board and rejected Niti advice against doing so. Writer Nitasha Kaul was not allowed to enter Bengaluru, allegedly detained and deported to London.\n",
            "11. Business: Relax, weâ€™ve got experts to help you set up a Mac for your business.\n",
            "12. Politics: Michael Sandel discusses whether the Opposition's politics can address aspirations expressed by Prime Minister Narendra Modi with national pride.\n",
            "\n",
            "----------\n",
            " 36.835289775867906  tokens/s\n",
            "Total number of tokens:  399\n",
            "Total time taken:  10.832003831863403\n"
          ]
        }
      ]
    }
  ]
}