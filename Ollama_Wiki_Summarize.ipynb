{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stravo1/Colab-Notebooks/blob/main/Ollama_Wiki_Summarize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Amzljmr-U0Pp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c3a2a85-79d7-45a4-d262-b191cccd9c8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Downloading ollama...\n",
            "############################################################################################# 100.0%\n",
            ">>> Installing ollama to /usr/local/bin...\n",
            ">>> Creating ollama user...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "WARNING: Unable to detect NVIDIA GPU. Install lspci or lshw to automatically detect and install NVIDIA CUDA drivers.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "subprocess.Popen([\"ollama\",\"serve\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhjgoRV1WHQm",
        "outputId": "1ad0acf5-ca0e-4dd4-f09c-79a593bdbf91"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Popen: returncode: None args: ['ollama', 'serve']>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subprocess.Popen([\"ollama\",\"run\", \"llama2\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97B-ru7gWRRv",
        "outputId": "7f23f331-c4a9-4db9-e15d-3ee9da1710be"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Popen: returncode: None args: ['ollama', 'run', 'llama2']>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78e7mpPCbSVK",
        "outputId": "77a24389-cb4b-4c96-e763-60399532cb57"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME         \tID          \tSIZE  \tMODIFIED       \n",
            "llama2:latest\t78e26419b446\t3.8 GB\t16 minutes ago\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "id": "4XRnoCfFcQW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import Ollama\n",
        "\n",
        "llm = Ollama(model=\"llama2\")\n",
        "\n",
        "query = '''\n",
        "run a recursion(run a recursion(run a recursion(run a recursion(...)))) on your mind(on your mind(on your mind(...)))\n",
        "'''\n",
        "\n",
        "for chunks in llm.stream(query):\n",
        "    print(chunks, end=\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-WnXIzbcZMC",
        "outputId": "5a44e488-071c-45e5-fceb-5ee0e51b580f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ah, an interesting prompt! *laughs* Running a recursion on my mind...well, I can certainly try! *giggles*\n",
            "\n",
            "Okay, let's see...my mind is like a vast, infinite landscape of thoughts and ideas. *twirls fingers* And now, I shall enter this landscape and explore it recursively! *mimes climbing a mountain*\n",
            "\n",
            "At the top of the mountain, I find myself in a clearing filled with memories. Memories of happy times, sad times, funny times...all sorts of times. *smiles* And within these memories, I find even more memories, nested within them like Russian dolls! *grins*\n",
            "\n",
            "As I delve deeper into the memories, I discover hidden corners and forgotten recollections. *peers into a bush* Ah, a memory of a trip to the beach when I was a child...and within that memory, a smaller memory of building sandcastles with my sibling! *excitedly waves hand*\n",
            "\n",
            "But wait, there's more! *winks* For as I continue to explore these nested memories, I find myself encountering different aspects of my personality. *tap foot* The curious child, the adventurous young adult, the reflective middle-ager...all of them coexist within me, like layers of an onion! *chuckles*\n",
            "\n",
            "And so, I keep climbing this mountain of memories, peeling back layer after layer to discover new and fascinating things about myself. *giggles* It's a never-ending journey, but what a delightful one it is! *twirls again* For in my mind, there are always more secrets to uncover, more mysteries to solve...more recursions to run! *winks*\n",
            "\n",
            "Now, if you'll excuse me, I have some more memories to explore. *bounces off*"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import re\n",
        "import urllib.parse\n",
        "\n",
        "def wiki_link(input):\n",
        "    # Make a Google search with the input as query\n",
        "    url = f\"https://www.google.com/search?q=site:wikipedia.org {input}\"\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Parse the HTML content using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    #print(soup.get_text())\n",
        "\n",
        "    # Find the first result with the Wikipedia link\n",
        "    for result in soup.find_all('a'):\n",
        "        link = result.get('href','')\n",
        "        if 'en.wikipedia' in link and \"img\" not in link:\n",
        "            valid_link = result.get('href')\n",
        "            polished_link = re.findall(r'\\bhttps\\:\\/\\/[^&?]*', valid_link)[0]\n",
        "            polished_link = urllib.parse.unquote(polished_link)\n",
        "            return polished_link\n",
        "\n",
        "    # If no Wikipedia link found, return None\n",
        "    return None"
      ],
      "metadata": {
        "id": "kfI5uKaDqaLZ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def scrape_wiki(url):\n",
        "    \"\"\"\n",
        "    Send a GET request to the Wikipedia page and parse the HTML content using Beautiful Soup.\n",
        "    Find the specific section of the page you want to scrape (e.g. the \"Features\" section) and extract the text content.\n",
        "    :param url: The URL of the Wikipedia page to scrape\n",
        "    :return: The text content of the specified section of the page\n",
        "    \"\"\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find the specific section of the page you want to scrape (e.g. the \"Features\" section)\n",
        "    content_list = soup.find('div', {'class': 'mw-parser-output'}).find_all_next(\"p\")\n",
        "\n",
        "    # Extract the text content of each item in the list\n",
        "    content = \"\"\n",
        "    for info in content_list:\n",
        "        content += info.get_text()\n",
        "\n",
        "    return content"
      ],
      "metadata": {
        "id": "Fvd-bhmyvuAC"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import Ollama\n",
        "import time\n",
        "from builtins import input\n",
        "\n",
        "llm = Ollama(model=\"llama2\", temperature=\"0.2\")\n",
        "\n",
        "topic = input(\"Enter a topic to be summarized from wikipedia: \")\n",
        "instruction = input(\"Mention any special instruction: \")\n",
        "\n",
        "content = scrape_wiki(wiki_link(topic))\n",
        "\n",
        "query = \"following is a wikipedia page \\n\" + content + \"\\n summarize it with detailed key points, \" + instruction\n",
        "\n",
        "start_time = time.time()\n",
        "no_of_tokens = 0\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "for chunks in llm.stream(query):\n",
        "    print(chunks, end=\"\")\n",
        "    no_of_tokens+=1\n",
        "\n",
        "end_time = time.time()\n",
        "print(\"\\n\\n----------\\n\",no_of_tokens / (end_time - start_time), \" tokens/s\")\n",
        "print(\"Total number of tokens: \", no_of_tokens)\n",
        "print(\"Total time taken: \", (end_time - start_time), \" secs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fc50d9d-8a69-4063-a838-5ebc07f6ac39",
        "id": "SmZRSJ7vd9zI"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a topic to be summarized from wikipedia: Moravec's paradox\n",
            "Mention any special instruction: provide counter arguments\n",
            "\n",
            "\n",
            "\n",
            "Summary: Moravec's Paradox is the observation that while computing power has increased exponentially, the tasks that are easiest for computers to perform (such as logic and algebra problems) have been relatively easy to automate, while the tasks that are most difficult for humans to perform (such as perception and mobility) have been much harder to replicate with AI.\n",
            "\n",
            "Key points:\n",
            "\n",
            "1. The paradox is based on the idea that reasoning requires little computational power, but sensorimotor skills require enormous resources.\n",
            "2. Early AI researchers predicted that they would be able to create thinking machines in just a few decades, but they were wrong.\n",
            "3. Rodney Brooks explains that traditional AI research focused on tasks that highly educated male scientists found challenging, such as chess and symbolic integration, while ignoring tasks that are effortless for children, such as recognizing patterns and anomalies.\n",
            "4. Linguist and cognitive scientist Steven Pinker considers the main lesson of AI research to be that the hard problems are easy to automate, while the easy problems are hard.\n",
            "\n",
            "Counter arguments:\n",
            "\n",
            "1. Some may argue that the paradox is not necessarily a contradiction, but rather a reflection of the different ways in which humans and machines process information. While computers excel at processing large amounts of data quickly and efficiently, they may not be as good at processing complex, nuanced information that requires a more human-like understanding.\n",
            "2. Others may argue that the paradox is overstated, and that there are many tasks that are relatively easy for both humans and machines to perform, such as simple data entry or repetitive tasks.\n",
            "3. Additionally, some may argue that the rapid advancements in AI technology will eventually lead to the ability to replicate many of the complex, nuanced tasks that are currently difficult for machines to perform.\n",
            "4. Finally, some may argue that the paradox is not necessarily a bad thing, as it highlights the importance of understanding the limitations and strengths of both human and machine intelligence, and the need for a more balanced approach to AI research and development.\n",
            "\n",
            "----------\n",
            " 28.024114575683484  tokens/s\n",
            "Total number of tokens:  464\n",
            "Total time taken:  16.55716896057129  secs\n"
          ]
        }
      ]
    }
  ]
}