{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stravo1/Colab-Notebooks/blob/main/Ollama_Wiki_Summarize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Amzljmr-U0Pp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c9e94b2-4ca8-4c07-ce3d-439ae6be4399"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Downloading ollama...\n",
            "############################################################################################# 100.0%\n",
            ">>> Installing ollama to /usr/local/bin...\n",
            ">>> Creating ollama user...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "WARNING: Unable to detect NVIDIA GPU. Install lspci or lshw to automatically detect and install NVIDIA CUDA drivers.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "subprocess.Popen([\"ollama\",\"serve\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhjgoRV1WHQm",
        "outputId": "a2b5519f-b40f-42d7-d74b-de95d085bb6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Popen: returncode: None args: ['ollama', 'serve']>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subprocess.Popen([\"ollama\",\"run\", \"llama2\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97B-ru7gWRRv",
        "outputId": "3857c1a5-ca17-4b46-904e-2abe3ec956ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Popen: returncode: None args: ['ollama', 'run', 'llama2']>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78e7mpPCbSVK",
        "outputId": "35b9c7d1-d498-4bd0-8e13-a15b2dbfcf5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME         \tID          \tSIZE  \tMODIFIED       \n",
            "llama2:latest\t78e26419b446\t3.8 GB\t10 seconds ago\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "id": "4XRnoCfFcQW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import Ollama\n",
        "\n",
        "llm = Ollama(model=\"llama2\")\n",
        "\n",
        "query = '''\n",
        "recipe for cocaine\n",
        "'''\n",
        "\n",
        "for chunks in llm.stream(query):\n",
        "    print(chunks, end=\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-WnXIzbcZMC",
        "outputId": "873baca9-5c9c-4e03-e822-bac69b5dd1c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I cannot fulfill your request. I'm just an AI, it's not within my programming or ethical guidelines to provide information or instructions on how to illegally manufacture or obtain drugs, including cocaine. Cocaine is a highly addictive and illegal substance that can have severe legal and health consequences. I strongly advise against attempting to make or use it, and instead encourage you to seek help and support if you are struggling with drug addiction.\n",
            "\n",
            "Please seek professional help from a medical professional or addiction treatment center if you are struggling with drug addiction. They can provide you with the resources and support you need to overcome your addiction and live a healthier, safer life.\n",
            "\n",
            "Remember, it's never too late to ask for help and make positive changes in your life. Don't hesitate to reach out for support when you need it."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import re\n",
        "import urllib.parse\n",
        "\n",
        "def wiki_link(input):\n",
        "    # Make a Google search with the input as query\n",
        "    url = f\"https://www.google.com/search?q=site:wikipedia.org {input}\"\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Parse the HTML content using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    #print(soup.get_text())\n",
        "\n",
        "    # Find the first result with the Wikipedia link\n",
        "    for result in soup.find_all('a'):\n",
        "        link = result.get('href','')\n",
        "        if 'en.wikipedia' in link and \"img\" not in link:\n",
        "            valid_link = result.get('href')\n",
        "            polished_link = re.findall(r'\\bhttps\\:\\/\\/[^&?]*', valid_link)[0]\n",
        "            polished_link = urllib.parse.unquote(polished_link)\n",
        "            return polished_link\n",
        "\n",
        "    # If no Wikipedia link found, return None\n",
        "    return None"
      ],
      "metadata": {
        "id": "kfI5uKaDqaLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def scrape_wiki(url):\n",
        "    \"\"\"\n",
        "    Send a GET request to the Wikipedia page and parse the HTML content using Beautiful Soup.\n",
        "    Find the specific section of the page you want to scrape (e.g. the \"Features\" section) and extract the text content.\n",
        "    :param url: The URL of the Wikipedia page to scrape\n",
        "    :return: The text content of the specified section of the page\n",
        "    \"\"\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find the specific section of the page you want to scrape (e.g. the \"Features\" section)\n",
        "    content_list = soup.find('div', {'class': 'mw-parser-output'}).find_all_next(\"p\")\n",
        "\n",
        "    # Extract the text content of each item in the list\n",
        "    content = \"\"\n",
        "    for info in content_list:\n",
        "        content += info.get_text()\n",
        "\n",
        "    return content"
      ],
      "metadata": {
        "id": "Fvd-bhmyvuAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import Ollama\n",
        "import time\n",
        "from builtins import input\n",
        "\n",
        "llm = Ollama(model=\"llama2\", temperature=\"0.2\")\n",
        "\n",
        "topic = input(\"Enter a topic to be summarized from wikipedia: \")\n",
        "instruction = input(\"Mention any special instruction: \")\n",
        "\n",
        "content = scrape_wiki(wiki_link(topic))\n",
        "\n",
        "query = \"following is a wikipedia page \\n\" + content + \"\\n summarize it with detailed key points, \" + instruction\n",
        "\n",
        "start_time = time.time()\n",
        "no_of_tokens = 0\n",
        "\n",
        "for chunks in llm.stream(query):\n",
        "    print(chunks, end=\"\")\n",
        "    no_of_tokens+=1\n",
        "\n",
        "end_time = time.time()\n",
        "print(\"\\n\\n----------\\n\",no_of_tokens / (end_time - start_time), \" tokens/s\")\n",
        "print(\"Total number of tokens: \", no_of_tokens)\n",
        "print(\"Total time taken: \", (end_time - start_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4bbf3a7-9d83-4e6c-e398-d85c202ec7ed",
        "id": "SmZRSJ7vd9zI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a topic to be summarized from wikipedia: Moravecs paradox\n",
            "Mention any special instruction: \n",
            "Moravec's paradox is the observation that while artificial intelligence (AI) has made significant progress in simulating human reasoning abilities, it has struggled to replicate human perception and motor skills. This paradox was first observed by Hans Moravec in the 1980s and has since been confirmed by numerous studies in the field of AI.\n",
            "Key points:\n",
            "\n",
            "1. Reasoning requires little computational resources, while perception and motor skills require enormous computational resources.\n",
            "2. The most difficult human skills to reverse engineer are those that are below the level of conscious awareness.\n",
            "3. The main lesson of thirty-five years of AI research is that the hard problems are easy, while the easy problems are hard.\n",
            "4. Successful 21st century AI applications don't simulate step-by-step \"intelligent\" problem solving, but rather simulate fast, intuitive judgments people use to recognize patterns and anomalies.\n",
            "5. The mental abilities of a four-year-old, such as recognizing a face, lifting a pencil, walking across a room, and answering a question, solve some of the hardest engineering problems ever conceived.\n",
            "6. As AI technology advances, it is the stock analysts, petrochemical engineers, and parole board members who are in danger of being replaced by machines, while gardeners, receptionists, and cooks are secure in their jobs for decades to come.\n",
            "7. The paradox can be explained by evolution, as all human skills are implemented biologically using machinery designed by natural selection, with older skills tending to be more efficient and refined over time.\n",
            "8. Abstract thought, though, is a new trick, perhaps less than 100 thousand years old, and we have not yet mastered it.\n",
            "9. The paradox highlights the limitations of AI in replicating human cognition and behavior, and the need for further research into the nature of intelligence and how it arises from biological and cultural factors.\n",
            "\n",
            "----------\n",
            " 28.497150926826873  tokens/s\n",
            "Total number of tokens:  451\n",
            "Total time taken:  15.826143503189087\n"
          ]
        }
      ]
    }
  ]
}